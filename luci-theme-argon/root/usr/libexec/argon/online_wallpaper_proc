#!/bin/bash
# author jjm2473
# modify author sun.wang fork to https://github.com/sunowsir/luci-theme-argon

date +"%Y-%m-%d %H:%M:%S"

G_WEB_PIC_SRC=$(uci -q get argon.@global[0].online_wallpaper || echo 'bing')
G_API_KEY=$(uci -q get argon.@global[0].use_api_key)
G_CURL_MAXTIME=$(uci -q get argon.@global[0].get_timeout || echo '30')
G_RESOLUTION_X=$(uci -q get argon.@global[0].reso_x || echo '2560')
G_RESOLUTION_Y=$(uci -q get argon.@global[0].reso_y || echo '1440')

# 图片存储路径, web js 代码会从该路径获取bg
G_BACKGROUND_IMG_FPATH='/www/luci-static/argon/img/bg.jpg'

# 最新URL缓存
G_CACHE="/var/run/argon_${G_WEB_PIC_SRC}.url"

# 运行时URL缓存
G_RUNNING_CACHE='/var/run/argon_running_bg.url'

# 锁文件
G_WRLOCK='/var/run/online_wallpaper_proc.lock'

# 默认一小时刷新一次
G_REFRESH_INTERVAL_SEC=$((1 * 60 * 60))

# 全局图片URL存储
G_IMG_URL=''


function fetch_pic_url_bing() {
    local picpath
    picpath=$(curl -fks --max-time "${G_CURL_MAXTIME}" \
        "https://www.bing.com/HPImageArchive.aspx?format=js&n=1&uhd=1" | \
        jsonfilter -qe '@.images[0].url')

    if [[ ! -n "${picpath}" ]]; then
        return 127
    fi

    G_IMG_URL="$(echo "//www.bing.com${picpath}" | sed "s/w=[0-9]*/${G_RESOLUTION_X}/g" | \
        sed "s/h=[0-9]*/${G_RESOLUTION_Y}/g" | tr -d '\r' | tr -d '\n' | sed 's/^\/\///g' )"

    return $?
}

function fetch_pic_url_unsplash() {
    if [[ -n "${G_API_KEY}" ]]; then
        G_IMG_URL="$(curl -fks --max-time "${G_CURL_MAXTIME}" \
            "https://api.unsplash.com/photos/random?client_id=${G_API_KEY}" | \
            jsonfilter -qe '@["urls"]["regular"]' | tr -d '\r' | tr -d '\n')"
    else
        G_IMG_URL="$(curl -fks --max-time "${G_CURL_MAXTIME}" \
            --header "Authorization: Client-ID kmFIroj2ELqXJPtC0XUoyww-Tr_lDU8Ho8uxjptIrCo" \
            "https://api.unsplash.com/photos/random?count=1&orientation=landscape" | \
            jsonfilter -e "@[0]['urls']['regular']" | tr -d '\r' | tr -d '\n')"
    fi

    if [[ -n "${G_IMG_URL}" ]]; then
        return 127
    fi

    return $?
}

function fetch_pic_url_pexels() {
    if [[ -z "${G_API_KEY}" ]]; then
        return 
    fi

    local result
    result=$(curl -s --max-time "${G_CURL_MAXTIME}" -H "Authorization: ${G_API_KEY}" \
        "https://api.pexels.com/v1/curated?per_page=80&page=1")

    if [[ "${result}" == "" ]]; then
        return 127
    fi

    local per_page
    per_page=$(echo "${result}" | jsonfilter -qe '@["per_page"]')

    for ((idx = 0; idx < per_page; idx++));
    do
        local random_idx=$((RANDOM % RANDOM % per_page))

        local node_filter
        node_filter='@["photos"]['${random_idx}']'

        local width 
        width=$(echo "${result}" | jsonfilter -qe "${node_filter}"'.width')
        if [[ ${width} -lt ${G_RESOLUTION_X} ]]; then
            continue
        fi

        local height
        height=$(echo "${result}" | jsonfilter -qe "${node_filter}"'.height')
        if [[ ${height} -lt ${G_RESOLUTION_Y} ]]; then
            continue
        fi

        if [[ ${width} -lt ${height} ]]; then
            continue
        fi

        G_IMG_URL="$(echo "${result}" | jsonfilter -qe "${node_filter}"'.src.original' | tr -d '\r' | tr -d '\n')"

        break;
    done

    return $?
}

function fetch_pic_url_picsum() {
    G_IMG_URL="$(curl --max-time "${G_CURL_MAXTIME}" -s -D - -o /dev/null "https://picsum.photos/${G_RESOLUTION_X}/${G_RESOLUTION_Y}" | \
        grep -i location: | tr -s ' ' | awk '{printf("%s", $2);}' | tr -d '\r' | tr -d '\n')"
    return $?
}

function fetch_pic_url() {
    case ${G_WEB_PIC_SRC} in
    bing)
        fetch_pic_url_bing
        ;;
    unsplash)
        fetch_pic_url_unsplash
        ;;
    pexels)
        fetch_pic_url_pexels
        ;;
    picsum)
        fetch_pic_url_picsum
        ;;
    esac

    return $?
}

function try_update() {
    local lock="${G_WRLOCK}"
    exec 200> "${lock}"

    if flock -n 200 >/dev/null 2>&1; then
        fetch_pic_url

        # url 非空 优先用url
        if [[ -n "${G_IMG_URL}" ]]; then
            echo -n "${G_IMG_URL}" > "${G_CACHE}"

            flock -u 200 >/dev/null 2>&1
            return $?
        fi

        # url 为空, 退出判断分支, 解锁
        flock -u 200 >/dev/null 2>&1
    fi

    # 加锁失败或url获取失败，直接取cache
    if [[ -s "${G_CACHE}" ]]; then 
        G_IMG_URL=$(cat "${G_CACHE}")
    fi

    return 127
}

function get_url() {
    if [[ -s "${G_CACHE}" ]] && [[ -s "${G_RUNNING_CACHE}" ]]; then
        local idle_t=$(($(date '+%s') - $(date -r "${G_CACHE}" '+%s' 2>/dev/null || echo '0')))

        # 获取URL后，第一时间保存到该cache文件中
        local cache_url
        cache_url=$(cat "${G_CACHE}" | tr -d '\n' | tr -d '\r')

        # 脚本执行到最后，将最终生效URL保存到 running cache 文件中
        local running_cache_url
        running_cache_url=$(cat "${G_RUNNING_CACHE}" | tr -d '\n' | tr -d '\r')

        # 未到刷新时间或缓存url与运行时url一致，则无需更新url
        if [[ ${idle_t} -le ${G_REFRESH_INTERVAL_SEC} ]] && 
           [[ "${cache_url}" == "${running_cache_url}" ]]; then
            G_IMG_URL=''
            return $?
        fi
    fi

    try_update
    return $?
}

function img_down() {
    local img_url
    img_url="${1}"

    rm -rf "${G_BACKGROUND_IMG_FPATH}"

    curl --http1.1 -L -fks --max-time "${G_CURL_MAXTIME}" \
        -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36" \
    --url "${img_url}" \
    -o "${G_BACKGROUND_IMG_FPATH}"

    return $?
}

function main() {
    get_url

    # 获取URL成功，写入URL到running cache 文件中, 并下载图片, 提高web响应速度
    if [[ -n "${G_IMG_URL}" ]]; then
        echo -n "${G_IMG_URL}" > "${G_RUNNING_CACHE}"
        img_down "${G_IMG_URL}"
    fi

    # 下载的背景图不存在，则无论如何都尝试下载
    if [[ ! -s "${G_BACKGROUND_IMG_FPATH}" ]]; then
        img_down "$(cat ${G_RUNNING_CACHE} | tr -d '\r' | tr -d '\n')"
    fi

    return $?
}

main "${@}"
exit $?


