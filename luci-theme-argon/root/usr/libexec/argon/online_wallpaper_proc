#!/bin/bash
# author jjm2473
# modify author sun.wang fork to https://github.com/sunowsir/luci-theme-argon

WEB_PIC_SRC=$(uci -q get argon.@global[0].online_wallpaper || echo 'bing')
API_KEY=$(uci -q get argon.@global[0].use_api_key)
CURL_MAXTIME=$(uci -q get argon.@global[0].get_timeout || echo '30')
RESOLUTION_X=$(uci -q get argon.@global[0].reso_x || echo '2560')
RESOLUTION_Y=$(uci -q get argon.@global[0].reso_y || echo '1440')

# 图片存储路径, web js 代码会从该路径获取bg
BACKGROUND_IMG_FPATH='/www/luci-static/argon/img/bg.jpg'

CACHE=/var/run/argon_${WEB_PIC_SRC}.url
RUNNING_CACHE=/var/run/argon_running_bg.url

WRLOCK=/var/lock/argon_${WEB_PIC_SRC}.lock

# 默认一小时刷新一次
REFRESH_INTERVAL_SEC=$((1 * 60 * 60))

function fetch_pic_url_pexels() {
    if [[ -z "${API_KEY}" ]]; then
        return 
    fi

    local result
    result=$(curl -s --max-time "${CURL_MAXTIME}" -H "Authorization: ${API_KEY}" \
        "https://api.pexels.com/v1/curated?per_page=80&page=1")

    if [[ "${result}" == "" ]]; then
        return 
    fi

    local per_page
    per_page=$(echo "${result}" | jsonfilter -qe '@["per_page"]')

    for ((idx = 0; idx < per_page; idx++));
    do
        local random_idx=$((RANDOM % RANDOM % per_page))

        local node_filter
        node_filter='@["photos"]['${random_idx}']'

        local width 
        width=$(echo "${result}" | jsonfilter -qe "${node_filter}"'.width')
        if [[ ${width} -lt ${RESOLUTION_X} ]]; then
            continue
        fi

        local height
        height=$(echo "${result}" | jsonfilter -qe "${node_filter}"'.height')
        if [[ ${height} -lt ${RESOLUTION_Y} ]]; then
            continue
        fi

        if [[ ${width} -lt ${height} ]]; then
            continue
        fi

        echo "${result}" | jsonfilter -qe "${node_filter}"'.src.original'

        break;
    done

    return 
}

function fetch_pic_url_picsum() {
    curl -s -D - -o /dev/null "https://picsum.photos/${RESOLUTION_X}/${RESOLUTION_Y}" | \
        grep -i location: | tr -s ' ' | awk '{printf("%s", $2);}'
}

function fetch_pic_url() {

    case ${WEB_PIC_SRC} in
    bing)
        declare picpath
        picpath=$(curl -fks --max-time "${CURL_MAXTIME}" \
            "https://www.bing.com/HPImageArchive.aspx?format=js&n=1&uhd=1" |
            jsonfilter -qe '@.images[0].url')
        [[ -n "${picpath}" ]] && echo "//www.bing.com${picpath}" | sed "s/w=[0-9]*/${RESOLUTION_X}/g" | sed "s/h=[0-9]*/${RESOLUTION_Y}/g"
        ;;
    unsplash)
        # fix from https://github.com/sbwml/luci-theme-argon
        if [[ -z "${API_KEY}" ]]; then
            curl -fks --max-time "${CURL_MAXTIME}" \
                --header "Authorization: Client-ID kmFIroj2ELqXJPtC0XUoyww-Tr_lDU8Ho8uxjptIrCo" \
                "https://api.unsplash.com/photos/random?count=1&orientation=landscape" |
                jsonfilter -e "@[0]['urls']['regular']"
        else
            curl -fks --max-time "${CURL_MAXTIME}" \
                "https://api.unsplash.com/photos/random?client_id=${API_KEY}" |
                jsonfilter -qe '@["urls"]["regular"]'
        fi
        ;;
    unsplash_*)
        local collection_id=${WEB_PIC_SRC#unsplash_}
        if [[ -z "${API_KEY}" ]]; then
            curl -fks --max-time "${CURL_MAXTIME}" \
                "https://source.unsplash.com/collection/${collection_id}/${RESOLUTION_X}x${RESOLUTION_Y}" |
                sed -E 's#^.*href="([^?]+)\?.*$#\1?fm=jpg\&fit=crop\&w='"${RESOLUTION_X}"'\&h='"${RESOLUTION_Y}"'#'
        else
            curl -fks --max-time "${CURL_MAXTIME}" \
                "https://api.unsplash.com/photos/random?client_id=${API_KEY}&collections=${collection_id}" |
                jsonfilter -qe '@["urls"]["regular"]'
        fi
        ;;
    pexels)
        fetch_pic_url_pexels
        ;;
    picsum)
        fetch_pic_url_picsum
        ;;
    esac
}

function try_update() {
    local lock="${WRLOCK}"
    exec 200> "${lock}"

    if flock -n 200 >/dev/null 2>&1; then
        local picurl
        picurl="$(fetch_pic_url | tr -d '\r')"

        # url 非空 优先用url
        if [[ -n "${picurl}" ]]; then
            rm -rf "${CACHE}"
            echo "${picurl}" | tee "${CACHE}"

            flock -u 200 >/dev/null 2>&1
            return 
        fi

        # url 为空, 退出判断分支, 解锁
        flock -u 200 >/dev/null 2>&1
    fi

    # 加锁失败或url获取失败，直接取cache
    if [[ -s "${CACHE}" ]]; then 
        cat "${CACHE}"
    fi

    return 
}

function get_url() {
    if [[ -f "${CACHE}" ]] && [[ -s "${CACHE}" ]] && 
       [[ -f "${RUNNING_CACHE}" ]] && [[ -s "${RUNNING_CACHE}" ]]; then
        local idle_t=$(($(date '+%s') - $(date -r "${CACHE}" '+%s' 2>/dev/null || echo '0')))

        # 获取URL后，第一时间保存到该cache文件中
        local cache_url
        cache_url=$(cat "${CACHE}" | tr -d '\n' | tr -d '\r')

        # 脚本执行到最后，将最终生效URL保存到 running cache 文件中
        local running_cache_url
        running_cache_url=$(cat "${RUNNING_CACHE}" | tr -d '\n' | tr -d '\r')

        if [[ ${idle_t} -le ${REFRESH_INTERVAL_SEC} ]] && 
           [[ "${cache_url}" == "${running_cache_url}" ]]; then
            return
        fi
    fi

    try_update
}

function img_down() {
    local img_url
    img_url="${1}"

    rm -rf "${BACKGROUND_IMG_FPATH}"

    curl --http1.1 -L -fks --max-time "${CURL_MAXTIME}" \
        -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36" \
    --url "${img_url}" \
    -o "${BACKGROUND_IMG_FPATH}"
}

function main() {
    local img_url 
    img_url="$(get_url | sed 's/^\/\///g')"

    # 获取URL成功，写入URL到running cache 文件中, 并下载图片, 提高web响应速度
    if [[ "${img_url}" != "" ]]; then
        echo "${img_url}" > "${RUNNING_CACHE}"
        img_down "${img_url}"
    fi

    # 下载的背景图不存在，则无论如何都尝试下载
    if [[ ! -f "${BACKGROUND_IMG_FPATH}" ]] || 
       [[ ! -s "${BACKGROUND_IMG_FPATH}" ]]; then
        img_down "$(cat ${RUNNING_CACHE} | tr -d '\r' | tr -d '\n')"
    fi

    cat "${RUNNING_CACHE}"
}

main "${@}"
exit $?

